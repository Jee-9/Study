{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet101_개빡쳐.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQWNp3Re9PO8vWip0GHGra",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jee-9/Study/blob/main/ResNet101_notworked.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrRySk79bu8f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "from torchvision import utils\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgWcIL-sbxhR",
        "outputId": "51687d91-03d4-4a3e-c8d8-cd7b8f21665d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omJBEAUdbztV"
      },
      "source": [
        "train_transformation = transforms.Compose([\n",
        "                                           transforms.Resize((224,224)),\n",
        "                                           transforms.ToTensor(),\n",
        "                                           transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder(root = '/content/drive/MyDrive/PROJECT SHARING FILES/imageData', transform = train_transformation)\n",
        "# torchvision is not from Facebook\n",
        "\n",
        "# train test split\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "train_size = int(0.8*len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size = 16, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(val_dataset, batch_size = 16, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7VHbDP65byX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1dfdcde-4223-4ab2-bc38-0505889a8ac1"
      },
      "source": [
        "for i, l in dataset:\n",
        "  print(i.shape)\n",
        "  print(l)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_GidvmWtRSv",
        "outputId": "980da91b-d9a5-4af7-aaa6-e8ad5024883f"
      },
      "source": [
        "for i, l in train_dl:\n",
        "  print(i.shape)\n",
        "  print(l)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 224, 224])\n",
            "tensor([2, 1, 3, 2, 0, 2, 1, 2, 2, 0, 0, 3, 3, 0, 0, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXpx4IrvcA3B"
      },
      "source": [
        "# from torchvision import datasets, models, transforms\n",
        "\n",
        "# train_size = int(0.8*len(dataset))\n",
        "# test_size = len(dataset) - train_size\n",
        "\n",
        "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYLVRf4gdJUd",
        "outputId": "52cb1ff5-ab39-4a0a-d7e1-0069a40473b7"
      },
      "source": [
        "print(len(dataset))\n",
        "print(len(val_dl)) # batch size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49836\n",
            "623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RYrF3jrPutr",
        "outputId": "a86f158b-571c-47a1-e979-d07658739518"
      },
      "source": [
        "print(train_dl.dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataset.Subset object at 0x7fd979978c50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wmHkHF4cMP8"
      },
      "source": [
        "class BottleNeck(nn.Module):\n",
        "  expansion = 4 # BottleNeck을 사용하는 50 레이어부터는 * 4 로 채널 올려줌\n",
        "  def __init__(self, in_channels, out_channels, stride=1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.residual_function = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False), # bias = False 주는 이유는 BatchNorm에서 bias 들어가서\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False), ####\n",
        "        # padding=1이 output size = output size 아닌가? 여기 padding = 'same' 안주고 1 주는 이유는 뭘까 ㅠㅠ \n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels * BottleNeck.expansion), # 64 * 4 = 256 , 128 * 4 = 512, 256 * 4 = 1024\n",
        "    )\n",
        "\n",
        "    self.shortcut = nn.Sequential() ## identity shortcut\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    if stride != 1 or in_channels != out_channels * BottleNeck.expansion: ## 가운데 conv2d가 stride != 1이라 변화하지 않는 경우를 말하는 듯\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=stride, bias=False), # 위와 같은 경우에는 shortcut 인자에 다음과 같은 연산을 ++++ 해준다\n",
        "          nn.BatchNorm2d(out_channels*BottleNeck.expansion) # BatchNorm 적용 out channels는 X4해서 내보냄.\n",
        "      ) ## projection shortcut\n",
        "      '''\n",
        "      projection shortcut 기법을 사용한 걸로 보이는데 대부분의 경우 identity shortcut보다 projection shortcut 기법을 사용하는 것이 더 정확하지만,\n",
        "      사실 모델의 복잡도가 크게 증가하는 방식이기 때문에 쓰지 않는 경우도 많이 있다고 합니다.\n",
        "      '''\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.residual_function(x) + self.shortcut(x)\n",
        "    x = self.relu(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo__ahCucRWR"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_block, num_classes=4, init_weights=True): \n",
        "    # init_weights : w의 초기값을 카이밍 히 normalization 사용하겠다는 뜻인거 같음, ResNet에서는 weight initialization 방법으로 he normalization 사용함.\n",
        "    '''\n",
        "    Tensorflow ResNet 모델 구현할 때 \n",
        "      x = Conv2D(filters=filter2, kernel_size=middle_kernel_size, padding='same', kernel_initializer='he_normal', name=conv_name_base+'2b')(x)\n",
        "      x = BatchNormalization(axis=3, name=bn_name_base+'2b')(x)\n",
        "      x = Activation('relu')(x)\n",
        "    다음과 같이 레이어 쌓는 것과 같은 원리이고 kernel_initializer = 'he_normal'을 여기서는 init_weights로 사용.\n",
        "    '''\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels=64\n",
        "    # 그림 보면 알겠지만 무조건 첫채널 64 !!\n",
        "\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    ) # 여기는 모든 크기의 ResNet에서 공통적인 시작 부분 , kernel_size = 7로 주는 게 특징임.\n",
        "\n",
        "    # Stacking layers\n",
        "    self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "    self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "    self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "    self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    if init_weights: # initialize weights : w 초기값 주는 방법\n",
        "      self._initialize_weights()\n",
        "\n",
        "  def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "    strides = [stride] + [1] * (num_blocks-1) # 이거 무슨 문법,,,,? ㅠ\n",
        "    layers=[]\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_channels, out_channels, stride))\n",
        "      self.in_channels = out_channels * block.expansion\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.conv1(x)\n",
        "    output = self.conv2_x(output)\n",
        "    x = self.conv3_x(output)\n",
        "    x = self.conv4_x(x)\n",
        "    x = self.conv5_x(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    '''\n",
        "    view 사용문법!\n",
        "    view(-1,3) -> (?,3) 행렬로 바꿔달라.\n",
        "    size(0) -> 행\n",
        "    -> 행 갯수를 그대로 유지하면서 flatten 시키는 건가?\n",
        "    '''\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "  def _initialize_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode = 'fan_out', nonlinearity='relu')\n",
        "        #### init.kaiming_normal_  = he_normalization\n",
        "        if m.bias is not None:\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, 0, 0.01)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "    # 그러니까 resnet이 weight 처음에 줄 때 카이밍 히 방법을 쓰는 건 알겠는데,,문법이ㅠㅎ (tensorflow에서 'he_normalize' 같은 거인듯) \n",
        "\n",
        "def resnet50():\n",
        "  return ResNet(BottleNeck, [3,4,6,3])\n",
        "\n",
        "def resnet101():\n",
        "  return ResNet(BottleNeck, [3,4,23,3])\n",
        "\n",
        "def resnet152():\n",
        "  return ResNet(BottleNeck, [3,8,36,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_OHeLYAcWYm",
        "outputId": "e84abee3-3eff-42a2-a530-729c4a18c068"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = resnet101().to(device)\n",
        "x = torch.randn(3,3,224,224).to(device)\n",
        "output = model(x)\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvFssPPZcYZE",
        "outputId": "861769e4-5e2d-4ea8-dc71-6a3eebad52c3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep 24 05:42:37 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    31W /  70W |   1840MiB / 15109MiB |     23%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lnn3PNgcZaO",
        "outputId": "1993744b-037f-4107-a787-f34cde397872"
      },
      "source": [
        "summary(model, (3,224,224), device = device.type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
            "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
            "             ReLU-15          [-1, 256, 56, 56]               0\n",
            "       BottleNeck-16          [-1, 256, 56, 56]               0\n",
            "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
            "             ReLU-19           [-1, 64, 56, 56]               0\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "             ReLU-22           [-1, 64, 56, 56]               0\n",
            "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
            "             ReLU-25          [-1, 256, 56, 56]               0\n",
            "       BottleNeck-26          [-1, 256, 56, 56]               0\n",
            "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
            "             ReLU-29           [-1, 64, 56, 56]               0\n",
            "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
            "             ReLU-32           [-1, 64, 56, 56]               0\n",
            "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
            "             ReLU-35          [-1, 256, 56, 56]               0\n",
            "       BottleNeck-36          [-1, 256, 56, 56]               0\n",
            "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
            "             ReLU-39          [-1, 128, 56, 56]               0\n",
            "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
            "             ReLU-42          [-1, 128, 28, 28]               0\n",
            "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
            "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-47          [-1, 512, 28, 28]               0\n",
            "       BottleNeck-48          [-1, 512, 28, 28]               0\n",
            "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
            "             ReLU-51          [-1, 128, 28, 28]               0\n",
            "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
            "             ReLU-54          [-1, 128, 28, 28]               0\n",
            "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-57          [-1, 512, 28, 28]               0\n",
            "       BottleNeck-58          [-1, 512, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "             ReLU-61          [-1, 128, 28, 28]               0\n",
            "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "             ReLU-64          [-1, 128, 28, 28]               0\n",
            "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-67          [-1, 512, 28, 28]               0\n",
            "       BottleNeck-68          [-1, 512, 28, 28]               0\n",
            "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
            "             ReLU-71          [-1, 128, 28, 28]               0\n",
            "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
            "             ReLU-74          [-1, 128, 28, 28]               0\n",
            "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-77          [-1, 512, 28, 28]               0\n",
            "       BottleNeck-78          [-1, 512, 28, 28]               0\n",
            "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
            "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
            "             ReLU-81          [-1, 256, 28, 28]               0\n",
            "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
            "             ReLU-84          [-1, 256, 14, 14]               0\n",
            "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
            "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
            "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-89         [-1, 1024, 14, 14]               0\n",
            "       BottleNeck-90         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
            "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
            "             ReLU-93          [-1, 256, 14, 14]               0\n",
            "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
            "             ReLU-96          [-1, 256, 14, 14]               0\n",
            "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-99         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-100         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
            "            ReLU-103          [-1, 256, 14, 14]               0\n",
            "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
            "            ReLU-106          [-1, 256, 14, 14]               0\n",
            "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-109         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-110         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
            "            ReLU-113          [-1, 256, 14, 14]               0\n",
            "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
            "            ReLU-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-119         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-120         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
            "            ReLU-123          [-1, 256, 14, 14]               0\n",
            "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
            "            ReLU-126          [-1, 256, 14, 14]               0\n",
            "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-129         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-130         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
            "            ReLU-133          [-1, 256, 14, 14]               0\n",
            "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "            ReLU-136          [-1, 256, 14, 14]               0\n",
            "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-139         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-140         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
            "            ReLU-143          [-1, 256, 14, 14]               0\n",
            "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
            "            ReLU-146          [-1, 256, 14, 14]               0\n",
            "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-149         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-150         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
            "            ReLU-153          [-1, 256, 14, 14]               0\n",
            "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
            "            ReLU-156          [-1, 256, 14, 14]               0\n",
            "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-159         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-160         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
            "            ReLU-163          [-1, 256, 14, 14]               0\n",
            "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
            "            ReLU-166          [-1, 256, 14, 14]               0\n",
            "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-169         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-170         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
            "            ReLU-173          [-1, 256, 14, 14]               0\n",
            "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
            "            ReLU-176          [-1, 256, 14, 14]               0\n",
            "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-179         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-180         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "            ReLU-183          [-1, 256, 14, 14]               0\n",
            "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
            "            ReLU-186          [-1, 256, 14, 14]               0\n",
            "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-189         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-190         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
            "            ReLU-193          [-1, 256, 14, 14]               0\n",
            "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
            "            ReLU-196          [-1, 256, 14, 14]               0\n",
            "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-199         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-200         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
            "            ReLU-203          [-1, 256, 14, 14]               0\n",
            "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
            "            ReLU-206          [-1, 256, 14, 14]               0\n",
            "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-209         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-210         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
            "            ReLU-213          [-1, 256, 14, 14]               0\n",
            "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
            "            ReLU-216          [-1, 256, 14, 14]               0\n",
            "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-219         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-220         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
            "            ReLU-223          [-1, 256, 14, 14]               0\n",
            "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
            "            ReLU-226          [-1, 256, 14, 14]               0\n",
            "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-229         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-230         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
            "            ReLU-233          [-1, 256, 14, 14]               0\n",
            "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
            "            ReLU-236          [-1, 256, 14, 14]               0\n",
            "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-239         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-240         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
            "            ReLU-243          [-1, 256, 14, 14]               0\n",
            "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
            "            ReLU-246          [-1, 256, 14, 14]               0\n",
            "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-249         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-250         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
            "            ReLU-253          [-1, 256, 14, 14]               0\n",
            "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
            "            ReLU-256          [-1, 256, 14, 14]               0\n",
            "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-259         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-260         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
            "            ReLU-263          [-1, 256, 14, 14]               0\n",
            "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
            "            ReLU-266          [-1, 256, 14, 14]               0\n",
            "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-269         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-270         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
            "            ReLU-273          [-1, 256, 14, 14]               0\n",
            "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
            "            ReLU-276          [-1, 256, 14, 14]               0\n",
            "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-279         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-280         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
            "            ReLU-283          [-1, 256, 14, 14]               0\n",
            "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
            "            ReLU-286          [-1, 256, 14, 14]               0\n",
            "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-289         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-290         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
            "            ReLU-293          [-1, 256, 14, 14]               0\n",
            "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
            "            ReLU-296          [-1, 256, 14, 14]               0\n",
            "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-299         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-300         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
            "            ReLU-303          [-1, 256, 14, 14]               0\n",
            "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
            "            ReLU-306          [-1, 256, 14, 14]               0\n",
            "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-309         [-1, 1024, 14, 14]               0\n",
            "      BottleNeck-310         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-311          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-312          [-1, 512, 14, 14]           1,024\n",
            "            ReLU-313          [-1, 512, 14, 14]               0\n",
            "          Conv2d-314            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-315            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-316            [-1, 512, 7, 7]               0\n",
            "          Conv2d-317           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-318           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-319           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-320           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-321           [-1, 2048, 7, 7]               0\n",
            "      BottleNeck-322           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-323            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-324            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-325            [-1, 512, 7, 7]               0\n",
            "          Conv2d-326            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-327            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-328            [-1, 512, 7, 7]               0\n",
            "          Conv2d-329           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-330           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-331           [-1, 2048, 7, 7]               0\n",
            "      BottleNeck-332           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-333            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-334            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-335            [-1, 512, 7, 7]               0\n",
            "          Conv2d-336            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-337            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-338            [-1, 512, 7, 7]               0\n",
            "          Conv2d-339           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-340           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-341           [-1, 2048, 7, 7]               0\n",
            "      BottleNeck-342           [-1, 2048, 7, 7]               0\n",
            "AdaptiveAvgPool2d-343           [-1, 2048, 1, 1]               0\n",
            "          Linear-344                    [-1, 4]           8,196\n",
            "================================================================\n",
            "Total params: 42,508,356\n",
            "Trainable params: 42,508,356\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 429.72\n",
            "Params size (MB): 162.16\n",
            "Estimated Total Size (MB): 592.45\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZskAsxDccE6"
      },
      "source": [
        "# 손실함수, optimizer, lr_scheduler 정의\n",
        "loss_func = nn.CrossEntropyLoss(reduction = 'sum')\n",
        "opt = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10)\n",
        "\n",
        "# 현재 lr을 계산하는 함수\n",
        "def get_lr(opt):\n",
        "  for param_group in opt.param_groups:\n",
        "    return param_group['lr']\n",
        "\n",
        "# 배치당 loss와 metric을 계산하는 함수\n",
        "def metric_per_batch(output, target):\n",
        "  pred = output.argmax(1, keepdim=True)\n",
        "  corrects = pred.eq(target.view_as(pred)).sum().item()\n",
        "  return corrects\n",
        "\n",
        "def loss_per_batch(loss_func, output, target, opt=None):\n",
        "  loss = loss_func(output, target)\n",
        "  metric_b = metric_per_batch(output, target)\n",
        "\n",
        "  if opt is not None:\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "  return loss, metric_b\n",
        "\n",
        "\n",
        "# 에폭당 loss를 정의하는 함수\n",
        "def loss_per_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
        "  running_loss = 0.0\n",
        "  running_metric = 0.0\n",
        "  len_data = len(dataset_dl.dataset)\n",
        "  # dataloader -> next(iter(dataloader))\n",
        "\n",
        "  for xb, yb in dataset_dl:\n",
        "    # xb = xb.to(device)\n",
        "    # yb = yb.to(device)\n",
        "    # output = model(xb)\n",
        "    print('output')\n",
        "\n",
        "    loss_b, metric_b = loss_per_batch(loss_func, output, yb, opt)\n",
        "\n",
        "    running_loss += loss_b\n",
        "\n",
        "    if metric_b is not None:\n",
        "      running_metric += metric_b\n",
        "\n",
        "    if sanity_check is True:\n",
        "      break\n",
        "\n",
        "  loss = running_loss / len_data\n",
        "  metric = running_metric / len_data\n",
        "  print('loss, metric')\n",
        "\n",
        "  return loss, metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnn1aaumXPOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c043da8-cefc-4945-eec2-bff529bcc42c"
      },
      "source": [
        "len(train_dl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2492"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZjdRcBqUzPV"
      },
      "source": [
        "def last_per_batch(model, loss_func, dataset_dl, opt=None):\n",
        "  running_loss = 0.0\n",
        "  running_metric = 0.0\n",
        "  len_data = len(dataset_dl.dataset)\n",
        "\n",
        "  for batch_idx, (data, target) in enumerate(dataset_dl):\n",
        "    data = data.cuda()\n",
        "    target = target.cuda()\n",
        "    output = model(data)\n",
        "\n",
        "    loss_b, metric_b = loss_per_batch(loss_func, output, target, opt)\n",
        "\n",
        "    running_loss += loss_b\n",
        "\n",
        "    if (batch_idx+1) % 200 :\n",
        "      loss_batch = running_loss / len_data\n",
        "      print('loss in 200 batches turn: ', loss_batch)\n",
        "      print(loss_b)\n",
        "\n",
        "    if metric_b is not None:\n",
        "      running_metric += metric_b\n",
        "\n",
        "\n",
        "\n",
        "  loss = running_loss / len_data\n",
        "  metric = running_metric / len_data\n",
        "  print('metric', metric)\n",
        "  print('epoch loss', loss)\n",
        "\n",
        "  return loss, metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbpXxjPSdUvt"
      },
      "source": [
        "#!pi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE2ruT0kSPMq"
      },
      "source": [
        "def train_func(model, params):\n",
        "\n",
        "  num_epochs = params['num_epochs']\n",
        "  loss_func = params['loss_func']\n",
        "  opt = params['optimizer']\n",
        "  train_dl = params['train_dl']\n",
        "  val_dl = params['val_dl']\n",
        "  sanity_check = params['sanity_check']\n",
        "  lr_scheduler = params['lr_scheduler']\n",
        "  path2weights = params['path2weights']\n",
        "\n",
        "  loss_history = { 'train': [], 'val': [] }\n",
        "  metric_history = { 'train': [], 'val': [] } \n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    current_lr = get_lr(opt)\n",
        "    print('Epoch {}/{}, current lr = {}'.format(epoch, num_epochs-1, current_lr))\n",
        "\n",
        "    model.train()\n",
        "    print('model trained')\n",
        "\n",
        "    train_loss, train_metric = last_per_batch(model, loss_func, train_dl, opt)\n",
        "    loss_history['train'].append(train_loss)\n",
        "\n",
        "    # import json\n",
        "    # if len(loss_history[\"train\"]) > 100000\n",
        "    # with open(\"file_name\", \"w\") as file:\n",
        "    #   json.dump(file, loss_history[\"train\"])\n",
        "    # loss_historty[\"train\"].clear()\n",
        "\n",
        "    if len(loss_history['train']) > 100000:\n",
        "      with open(\"file_name\", \"w\") as file:\n",
        "      # the best way to open a file as read/write if it exists, or if it does not, then create it and open it as read/write\n",
        "        json.dump(file, loss_history['train'])\n",
        "        print('Train Loss : {}, Train Metric: {}'.format(train_loss, train_metric))\n",
        "      loss_history['train'].clear\n",
        "\n",
        "\n",
        "    # print('Train Loss : {}, Train Metric: {}'.format(train_loss, train_metric))\n",
        "\n",
        "\n",
        "  return loss_history, train_loss, train_metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQq7KUkAYJJ4"
      },
      "source": [
        "params_train = {\n",
        "    'num_epochs': 30,\n",
        "    'optimizer' : opt,\n",
        "    'loss_func' : loss_func,\n",
        "    'train_dl' : train_dl,\n",
        "    'val_dl' : val_dl,\n",
        "    'sanity_check' : False,\n",
        "    'lr_scheduler' : lr_scheduler,\n",
        "    'path2weights' : './models/weights.pt',\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rUG1oXxVWdSn",
        "outputId": "c7a1d242-2c5a-4c59-f830-781d90c514cf"
      },
      "source": [
        "train_func(model, params_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/29, current lr = 0.001\n",
            "model trained\n",
            "loss in 200 batches turn:  tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(27.3697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0015, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(30.9919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0025, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(39.9773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(35.6813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0043, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(38.2967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0050, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(26.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.3509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(19.7780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(21.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(20.6740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(55.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0094, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(44.5952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0103, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(36.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0110, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(26.5004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0115, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(21.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(29.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0128, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(23.3571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0133, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(19.2637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(18.8739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0143, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(21.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0149, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(24.4414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0154, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.8858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0157, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.6548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0161, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(17.4450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0165, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.8522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0172, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(26.8963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0176, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.4234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0179, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.4528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.6658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0191, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(20.9717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0196, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(19.4110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0202, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(22.7799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0206, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.9506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.2955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0213, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.6910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0215, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0218, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.7198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0222, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.4803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0224, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.3593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0227, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.6444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0231, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.4453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.9866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0236, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.3234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0243, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(25.7840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0248, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(20.7677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0251, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.5351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0254, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.9818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0256, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.9255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0260, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.6258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(23.8623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.7625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0274, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(19.9387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0277, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.7236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0280, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.7185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0284, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.6971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0289, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0292, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.4243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.9167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(19.4458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0305, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.3235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0309, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.7772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.4959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0313, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.3645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0315, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.5262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.8162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(27.9357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.4943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0333, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.2036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.2038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0341, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(19.1488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.3209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0348, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.9089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0359, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.5304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0364, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(20.6106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0369, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(19.4435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.7376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0376, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.5656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.9410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.6302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0385, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.5331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0387, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.3311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.8501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.9487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.9427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(22.4453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.5480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.2658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.7963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(22.5269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.9962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.5057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(20.9359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(21.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.8183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.9960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(23.6324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.6869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.9499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.2717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(21.3134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.7478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(21.3951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.7563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.6000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.4834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.7479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(19.8694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(17.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.3681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(30.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.7544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.8084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.4934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.4217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.5900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.2597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.9493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.5742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.9556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.4801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.8222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.9657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.6143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.6948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(17.7227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.5105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.4083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.8647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.4213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.7073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(20.1995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0618, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(26.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.8314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.7591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.4057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.4731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.4356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.5751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.4626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.9168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0648, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.4327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0650, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.6401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.5265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0657, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.6616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.2675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0663, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.9605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0666, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.7999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0668, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.9290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.7224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0675, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.9212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0678, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0682, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.7743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0685, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.8546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0687, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.7019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0691, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.9223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.8719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0696, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.7245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0698, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.4758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.6544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0704, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.5963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0707, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.9623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0709, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.7212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.9389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0715, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(17.2787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0717, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0719, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.4475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0720, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0722, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0724, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.4010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.2833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0727, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.6072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0735, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0737, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.6357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0742, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.3659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.7734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0749, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.9623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0751, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.7317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0753, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.9197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0756, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.2734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0757, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.4305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0761, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.9191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0763, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.9863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0766, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.2601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0772, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.9424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0775, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0779, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.3601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0781, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.4811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0783, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0787, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.9426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0789, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.6400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.7302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.8206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0797, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.1602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0798, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0801, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.6629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0803, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.7776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0806, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.6476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0809, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0814, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(17.9844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(23.8723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0825, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(20.5612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0827, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.8843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0830, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.9758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0834, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.7921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0837, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.6859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0842, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(19.4960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.7276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0848, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.9415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0853, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.4438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0857, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.2566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0858, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.7603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0860, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.3290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.6529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0867, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.8757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0870, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.5472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0874, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.8055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0875, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.8657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0878, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.8937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0879, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.6373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0887, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.6020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0888, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.9819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0889, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.7409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0894, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.6103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0899, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0900, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0902, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.5233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.8297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0908, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.3245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0910, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.4887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.9512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0913, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.4322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.5753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0919, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.9734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0922, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.8551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0923, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0925, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0927, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0928, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.7038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0928, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.9895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0930, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0933, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.6540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0936, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.8082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0936, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.3986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0941, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(18.3702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0944, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.6945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.1755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0949, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.8475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0951, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.4794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0954, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.4289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0959, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(19.5134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0962, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0964, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0965, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0967, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0969, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.3713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0971, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0972, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.3206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0976, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0977, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0978, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0980, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.7352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0982, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0985, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0987, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.3488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0991, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.3344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0992, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0994, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.6890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0996, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.9137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0997, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.0999, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.8872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1004, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.2773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1005, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1008, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.9076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1010, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.8285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1014, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1016, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.7853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.5596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1019, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1023, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.3661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1024, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.2874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1028, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.8460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1031, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.9532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.1831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.4924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1039, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.1235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.6882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1043, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1045, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1049, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.4259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.8407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.6006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1055, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1056, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1058, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.4434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1060, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.7023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1062, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.6040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.2825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1071, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.9207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1072, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.5514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.4144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1075, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.8081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1079, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.9987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1081, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.8145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1085, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.9577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1086, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.6083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1087, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1088, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.5194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1091, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1096, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(21.5853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1099, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.1679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1100, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1103, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.6483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.6522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1107, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.2957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1110, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.9997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1113, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.6022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.3890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1116, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.1934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1117, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1118, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1120, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1122, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.9713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1124, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1128, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.5106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1132, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.9718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1134, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.9659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.9947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1139, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.7648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.7422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.5374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.2166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.9526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1149, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.6675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1153, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.8084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1155, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.7978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1157, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.6482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1158, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.8941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1160, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.6635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1161, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.5650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1163, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1165, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.6505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1166, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.6874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1168, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.4928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1171, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.6067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1173, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1174, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1176, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.5287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1179, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.7077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1181, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.9260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1182, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.5810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.8157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1185, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1188, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1190, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.7787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1191, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.4315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1194, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.5551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1194, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.6562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1196, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.5631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1197, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.7560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1198, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1200, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1201, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.5300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1202, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.8488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1205, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.7035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1208, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.3132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1215, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(27.8894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1216, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1217, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.4862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.8110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1220, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1221, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.5131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1223, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1225, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1227, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.3554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1231, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.7620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1232, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.5173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.8209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1234, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1236, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.7524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1237, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1239, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.9159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1244, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1247, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.3380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1248, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1253, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.3948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1253, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.8919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1256, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.8142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1256, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1259, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.6081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1261, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.1235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1262, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1264, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1267, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.1917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.5343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1271, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.8161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1271, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1273, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1276, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.6028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1277, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.6243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1280, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.3596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1285, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.1293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.6676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.8669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1292, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.8352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1293, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1299, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.2695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1300, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1303, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.9222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1303, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.3668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1305, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.7619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1306, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.9930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1309, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.8822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1312, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1312, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1313, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1317, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.5107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.2714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1321, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1322, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1322, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.8909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1323, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1325, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.8998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1326, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1326, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1328, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1330, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.7190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1332, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.4614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1332, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.8089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(25.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1340, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1342, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.2218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1348, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.7448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1349, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.4782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1353, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1355, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.4829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.9141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1358, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.3348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1358, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.6425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1360, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1362, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.7606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1364, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.7861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1365, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1367, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.4547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1369, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.9981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1371, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1372, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1374, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.7934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1375, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1377, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.6254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1377, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.9062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1384, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(18.9514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1385, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1386, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.4928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1386, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1388, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.8527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1389, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1390, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1390, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.4608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.1624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1396, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.4696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.6985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1401, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.5341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1404, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.2130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1404, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.7055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1409, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1410, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1414, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.8004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.8979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1416, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1417, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1417, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.3099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1418, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1421, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.4045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1421, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.6767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1424, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1425, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.3315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1426, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.2124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1431, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(20.4561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1435, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.3510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1437, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.4927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1439, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.2521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1441, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.8664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1443, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.3598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1444, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.2974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1445, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.6094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1447, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.5832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1448, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.6567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1449, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.4992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1451, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.6406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1453, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.6748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1455, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.5282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1456, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1458, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1459, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1459, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.8413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1461, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.5977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.3707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1466, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.8637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1468, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.2025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1468, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1471, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.5459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.3599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1474, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.8610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1475, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.5347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1480, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.5609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1484, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1486, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.7576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.8374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1490, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.7286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1491, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.7632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1493, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.3523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1496, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1497, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.4010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1499, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.7755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1502, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.3618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1503, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1504, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1505, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.9811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1508, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.8598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1509, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1510, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.6625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1511, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1514, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1515, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1516, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1518, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.4019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1519, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.4662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1520, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1526, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(15.5503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1529, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.8444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1533, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.5334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1535, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.5046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1537, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.4054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1540, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1542, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.8593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.5113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1546, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.2732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1548, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.6251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1549, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1551, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.7906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1552, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1555, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1557, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.6353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1558, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.9203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1560, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1560, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.2061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1561, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.7983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1564, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.6335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1565, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1566, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1569, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1570, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1570, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.8200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1572, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.4924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1573, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.7101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1574, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.4527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1577, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.3712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.2982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1581, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(12.7927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1585, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(17.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1586, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.1428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1587, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.6379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.4898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1589, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1593, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(19.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1594, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.2918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.8268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.3439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1598, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.2926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1600, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.6180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1600, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.8447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1601, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1602, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.7105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1604, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.2851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1605, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.9458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1607, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.5639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1608, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.8936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1610, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.7873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1610, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1610, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.5924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1611, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1613, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1615, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1615, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1617, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.1622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1621, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.8964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1622, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1623, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.3934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1624, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.8261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1624, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1625, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1625, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.5024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1626, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.7869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1627, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.4105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1627, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.9485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1630, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.5742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1632, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.5118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1636, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.9049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1639, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.8342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1644, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(19.3574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1647, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.9939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1647, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1647, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.9805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1648, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1649, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1650, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.6511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1651, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.5076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.5375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1653, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.7995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1653, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.9399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.2001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1659, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.7299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1662, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.7543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1664, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.5876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1665, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1667, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.9922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1670, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.7082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1671, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.5822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.7572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1675, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.8369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.4494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1679, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1683, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.9484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1684, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.4301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1685, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1686, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.9085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.3596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1689, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.4149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1689, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1691, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.8449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1693, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.4015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1695, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1697, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1698, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.2156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.8686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1704, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.4800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1705, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.8786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1706, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1707, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.7248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.4244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.9224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1713, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.7630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1713, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.4193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1714, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.4052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1719, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.7878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1720, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1721, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1722, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.2174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1724, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1725, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.4956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1727, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1727, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.3639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.1199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1735, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1736, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1738, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1738, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.4640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.9789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1741, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.4576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1741, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1743, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.6554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1743, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.9765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.4127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.1917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1745, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.4569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1746, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.4470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.7104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1749, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.9808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1752, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.6071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1752, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1753, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.3971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1755, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1755, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.7872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1757, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.7490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1757, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1758, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.7807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.5859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1761, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.5345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1763, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.8440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1764, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.6356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1767, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.4946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1769, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.6674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1770, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1771, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1772, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.7047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1772, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.5323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1773, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1773, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.7036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1774, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1774, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1775, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1776, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1776, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.8043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1779, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.5781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1780, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1780, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.6594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1782, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.1866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1782, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.7269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1783, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1784, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.9263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1787, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.3599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1788, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.3486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.9768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1792, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.5910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1792, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.6717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1797, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.5606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1797, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.5308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1798, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.6196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1799, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1801, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.4113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.4408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1803, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.7111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1803, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.8937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1805, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1805, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.1800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1807, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.8648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1810, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.5591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1811, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1813, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1814, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1815, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.9040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1816, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.8691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1818, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.6689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.8824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.9558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1822, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1823, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1825, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.9856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1826, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1829, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.4589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1829, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1830, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.2105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.9744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1834, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.6649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1834, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1835, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1836, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1838, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.3140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1838, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.2127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1838, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.6188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.2938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1841, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.6556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1842, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1843, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.5300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.8621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.3340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.1206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1851, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.4466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1854, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.5406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1856, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.5769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1857, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.9927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1859, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1860, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1861, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.4799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.9110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1865, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.4946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.6257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1869, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1873, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.2831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1873, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.7429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1877, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(17.1254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1879, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1880, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1881, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.8469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.7430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.2154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1885, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.8545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1885, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.4732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1886, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1889, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.5660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.5559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.4892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1896, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.7398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1896, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.7828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1899, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1900, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1901, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1904, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.9992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1904, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.4541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.8669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.5414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.4255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1907, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.6361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1909, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.5751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1909, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.5027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1910, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1910, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.3175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.5496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.4772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1913, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1915, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1915, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.8394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.6884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1917, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1920, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1921, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1925, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(16.2002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1927, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.9198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1929, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.8875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1932, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.4064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1932, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.6168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1933, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.9704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1935, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.8469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1935, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.3767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1939, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.1406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1940, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.6816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1940, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.6122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1941, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.4009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1941, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.6758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1944, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.1900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1945, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.9693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.4217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1947, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.9899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1948, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1948, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1948, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1949, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1950, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.6228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1950, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.8248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1953, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.8012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1957, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1959, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1961, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.7339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1962, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.2867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1962, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1963, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1964, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1964, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.6570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1965, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1966, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.5018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.4019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1969, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.4936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.4970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1971, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1972, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.9389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1973, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.2600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.3176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1975, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.4719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1976, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1977, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.4785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.6512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.3337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1980, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.8730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1981, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.5970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1982, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.5249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1986, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(13.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1987, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.6908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1987, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.8215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1990, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(10.9305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1992, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.2547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1995, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.6826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1997, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.6929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.1998, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.3875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2000, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2001, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.9254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.9230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2005, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.2598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2007, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.9979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2007, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2010, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.3145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2010, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2010, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.8876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.2847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.7933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2012, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2013, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2014, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.8929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2014, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.6212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2016, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.7775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2018, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.6487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2019, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.5428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2020, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2022, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.1641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2023, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.4809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2023, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.4318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2024, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.6268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2024, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.8423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.1912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.9468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.1584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2030, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.3885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2030, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.1178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.1894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.7305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2039, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2040, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.3156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2043, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.8268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2043, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.1686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.9648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2045, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2046, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.8742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.8262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2048, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(7.5622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2050, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2050, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2050, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2051, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2051, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2051, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.2141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2055, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.4377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2055, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.6108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2055, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.7061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2056, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.8497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2057, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.3263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2057, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.4210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2058, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.6673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2060, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(4.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2061, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2061, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2061, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.5046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2062, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(5.2761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2063, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.9565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.1501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.9958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.3226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.6166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2068, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(8.8334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.5703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2071, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(9.5016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2072, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2073, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.5218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(14.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2079, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(11.2676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2080, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2080, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.7845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2080, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2081, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.1943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2081, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.1723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2081, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.4544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2083, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(6.3209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2083, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.8269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2084, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2084, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.3475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2085, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.2038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2085, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.9470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2086, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(2.3688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2086, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2087, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(1.5279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "loss in 200 batches turn:  tensor(0.2088, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(3.8127, device='cuda:0', grad_fn=<NllLossBackward>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ce12f2999ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-f9f28560d2b7>\u001b[0m in \u001b[0;36mtrain_func\u001b[0;34m(model, params)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model trained'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_per_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-7deab4e0a492>\u001b[0m in \u001b[0;36mlast_per_batch\u001b[0;34m(model, loss_func, dataset_dl, opt)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[1;32m    231\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2850\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrvk952khoA6"
      },
      "source": [
        "- reference : https://tutorials.pytorch.kr/beginner/former_torchies/nnft_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryNnomt5SoCe",
        "outputId": "bf6c761a-34b4-48c9-ae7d-df0f40ec4a5f"
      },
      "source": [
        "for batch_idx, (data, target) in enumerate(train_dl):\n",
        "  print(batch_idx)\n",
        "  print(data)\n",
        "  print(target)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "tensor([[[[-0.9838, -0.9782, -0.9843,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9739, -0.9826, -0.9903,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          [-0.9663, -0.9863, -0.9944,  ..., -0.9059, -0.9059, -0.9059],\n",
            "          ...,\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
            "\n",
            "         [[-0.9759, -0.9720, -0.9843,  ..., -0.9451, -0.9451, -0.9451],\n",
            "          [-0.9660, -0.9764, -0.9903,  ..., -0.9451, -0.9451, -0.9451],\n",
            "          [-0.9585, -0.9801, -0.9944,  ..., -0.9451, -0.9451, -0.9451],\n",
            "          ...,\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
            "\n",
            "         [[-1.0000, -1.0000, -1.0000,  ..., -0.9686, -0.9686, -0.9686],\n",
            "          [-0.9966, -1.0000, -1.0000,  ..., -0.9686, -0.9686, -0.9686],\n",
            "          [-0.9935, -0.9996, -0.9986,  ..., -0.9686, -0.9686, -0.9686],\n",
            "          ...,\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.4891, -0.4678, -0.4347,  ..., -0.8431, -0.8493, -0.8510],\n",
            "          [ 0.0608,  0.0813,  0.1128,  ..., -0.8431, -0.8493, -0.8510],\n",
            "          [ 0.6994,  0.7192,  0.7496,  ..., -0.8431, -0.8493, -0.8510],\n",
            "          ...,\n",
            "          [ 0.2654,  0.2260,  0.2070,  ..., -0.6272, -0.6901, -0.6858],\n",
            "          [ 0.3246,  0.2802,  0.2742,  ..., -0.5779, -0.6471, -0.6735],\n",
            "          [ 0.3535,  0.3064,  0.3137,  ..., -0.5345, -0.6112, -0.6678]],\n",
            "\n",
            "         [[-0.7636, -0.7457, -0.7249,  ..., -0.8588, -0.8650, -0.8667],\n",
            "          [-0.2756, -0.2569, -0.2320,  ..., -0.8588, -0.8650, -0.8667],\n",
            "          [ 0.2798,  0.2996,  0.3297,  ..., -0.8588, -0.8650, -0.8667],\n",
            "          ...,\n",
            "          [-0.1920, -0.2314, -0.2507,  ..., -0.8684, -0.9492, -0.9482],\n",
            "          [-0.1261, -0.1705, -0.1750,  ..., -0.8302, -0.9155, -0.9561],\n",
            "          [-0.0936, -0.1406, -0.1305,  ..., -0.7927, -0.8840, -0.9641]],\n",
            "\n",
            "         [[-0.9989, -0.9793, -0.9524,  ..., -0.8353, -0.8415, -0.8431],\n",
            "          [-0.5473, -0.5277, -0.4995,  ..., -0.8353, -0.8415, -0.8431],\n",
            "          [-0.0440, -0.0242,  0.0059,  ..., -0.8353, -0.8415, -0.8431],\n",
            "          ...,\n",
            "          [-0.3836, -0.4230, -0.4414,  ..., -0.8550, -0.9387, -0.9379],\n",
            "          [-0.2975, -0.3420, -0.3509,  ..., -0.8309, -0.9194, -0.9569],\n",
            "          [-0.2504, -0.2975, -0.2958,  ..., -0.8034, -0.8980, -0.9725]]],\n",
            "\n",
            "\n",
            "        [[[-0.3406, -0.3350, -0.3412,  ..., -0.5922, -0.6045, -0.6078],\n",
            "          [-0.3664, -0.3634, -0.3680,  ..., -0.5922, -0.6045, -0.6078],\n",
            "          [-0.4095, -0.4097, -0.4129,  ..., -0.5932, -0.6048, -0.6086],\n",
            "          ...,\n",
            "          [-0.5513, -0.5516, -0.5557,  ..., -0.4321, -0.5228, -0.4392],\n",
            "          [-0.5529, -0.5529, -0.5557,  ..., -0.4427, -0.5384, -0.4583],\n",
            "          [-0.5529, -0.5529, -0.5557,  ..., -0.4521, -0.5529, -0.4728]],\n",
            "\n",
            "         [[-0.3720, -0.3664, -0.3725,  ..., -0.4588, -0.4527, -0.4510],\n",
            "          [-0.4014, -0.3984, -0.4030,  ..., -0.4588, -0.4527, -0.4510],\n",
            "          [-0.4487, -0.4489, -0.4521,  ..., -0.4583, -0.4525, -0.4502],\n",
            "          ...,\n",
            "          [-0.1303, -0.1301, -0.1322,  ..., -0.4828, -0.5803, -0.5286],\n",
            "          [-0.1294, -0.1294, -0.1322,  ..., -0.5008, -0.6062, -0.5546],\n",
            "          [-0.1294, -0.1294, -0.1322,  ..., -0.5148, -0.6280, -0.5731]],\n",
            "\n",
            "         [[-0.4426, -0.4370, -0.4431,  ..., -0.7412, -0.7412, -0.7412],\n",
            "          [-0.4610, -0.4580, -0.4626,  ..., -0.7412, -0.7412, -0.7412],\n",
            "          [-0.4941, -0.4943, -0.4975,  ..., -0.7412, -0.7412, -0.7412],\n",
            "          ...,\n",
            "          [-0.3961, -0.3994, -0.4146,  ..., -0.5929, -0.6865, -0.6258],\n",
            "          [-0.3961, -0.3994, -0.4146,  ..., -0.5997, -0.6972, -0.6367],\n",
            "          [-0.3961, -0.3994, -0.4146,  ..., -0.6067, -0.7081, -0.6443]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.3317, -0.3098, -0.3126,  ..., -0.4639, -0.4728, -0.4818],\n",
            "          [-0.3423, -0.3163, -0.3162,  ..., -0.4711, -0.4830, -0.4927],\n",
            "          [-0.3525, -0.3270, -0.3212,  ..., -0.4829, -0.4995, -0.5104],\n",
            "          ...,\n",
            "          [-0.8039, -0.8039, -0.8039,  ..., -0.2531, -0.2382, -0.2304],\n",
            "          [-0.8039, -0.8039, -0.8039,  ..., -0.2464, -0.2300, -0.2165],\n",
            "          [-0.8039, -0.8039, -0.8039,  ..., -0.2415, -0.2263, -0.2095]],\n",
            "\n",
            "         [[-0.3473, -0.3255, -0.3283,  ..., -0.4560, -0.4650, -0.4739],\n",
            "          [-0.3580, -0.3320, -0.3319,  ..., -0.4633, -0.4751, -0.4849],\n",
            "          [-0.3682, -0.3427, -0.3369,  ..., -0.4750, -0.4917, -0.5025],\n",
            "          ...,\n",
            "          [-0.8039, -0.8039, -0.8039,  ..., -0.3158, -0.3009, -0.2932],\n",
            "          [-0.8039, -0.8039, -0.8039,  ..., -0.3091, -0.2927, -0.2793],\n",
            "          [-0.8039, -0.8039, -0.8039,  ..., -0.3042, -0.2891, -0.2723]],\n",
            "\n",
            "         [[-0.2611, -0.2392, -0.2364,  ..., -0.4403, -0.4493, -0.4583],\n",
            "          [-0.2717, -0.2457, -0.2401,  ..., -0.4476, -0.4594, -0.4692],\n",
            "          [-0.2819, -0.2560, -0.2439,  ..., -0.4576, -0.4757, -0.4868],\n",
            "          ...,\n",
            "          [-0.7882, -0.7882, -0.7882,  ..., -0.3315, -0.3221, -0.3166],\n",
            "          [-0.7882, -0.7882, -0.7882,  ..., -0.3248, -0.3146, -0.3028],\n",
            "          [-0.7882, -0.7882, -0.7882,  ..., -0.3199, -0.3109, -0.2958]]],\n",
            "\n",
            "\n",
            "        [[[-0.6291, -0.6936, -0.6880,  ..., -0.9199, -0.9020, -0.8611],\n",
            "          [-0.6733, -0.7950, -0.7865,  ..., -0.8224, -0.8949, -0.8788],\n",
            "          [-0.7276, -0.8911, -0.8541,  ..., -0.7139, -0.8865, -0.8975],\n",
            "          ...,\n",
            "          [-0.9999, -0.9934, -0.9785,  ..., -0.9124, -0.8690, -0.7669],\n",
            "          [-0.9961, -0.9977, -0.9917,  ..., -0.7906, -0.7840, -0.7156],\n",
            "          [-0.9927, -1.0000, -1.0000,  ..., -0.6936, -0.7143, -0.6739]],\n",
            "\n",
            "         [[-0.6000, -0.6936, -0.6683,  ..., -0.8073, -0.7798, -0.7137],\n",
            "          [-0.6369, -0.7893, -0.7721,  ..., -0.7097, -0.7728, -0.7314],\n",
            "          [-0.6797, -0.8800, -0.8482,  ..., -0.6022, -0.7644, -0.7525],\n",
            "          ...,\n",
            "          [-0.9912, -1.0000, -1.0000,  ..., -0.8476, -0.8000, -0.6965],\n",
            "          [-0.9966, -0.9991, -0.9958,  ..., -0.7136, -0.7033, -0.6366],\n",
            "          [-1.0000, -0.9983, -0.9922,  ..., -0.6106, -0.6263, -0.5877]],\n",
            "\n",
            "         [[-0.7087, -0.7877, -0.7737,  ..., -0.8157, -0.7860, -0.7289],\n",
            "          [-0.7386, -0.8790, -0.8663,  ..., -0.7181, -0.7790, -0.7465],\n",
            "          [-0.7706, -0.9607, -0.9270,  ..., -0.6097, -0.7706, -0.7669],\n",
            "          ...,\n",
            "          [-0.9616, -0.9616, -0.9641,  ..., -0.9020, -0.8410, -0.7210],\n",
            "          [-0.9530, -0.9608, -0.9636,  ..., -0.7850, -0.7593, -0.6738],\n",
            "          [-0.9462, -0.9608, -0.9636,  ..., -0.6919, -0.6924, -0.6359]]],\n",
            "\n",
            "\n",
            "        [[[-0.5294, -0.5294, -0.5294,  ..., -0.5860, -0.5669, -0.5395],\n",
            "          [-0.5242, -0.5242, -0.5242,  ..., -0.5764, -0.5562, -0.5289],\n",
            "          [-0.5128, -0.5128, -0.5128,  ..., -0.5556, -0.5330, -0.5059],\n",
            "          ...,\n",
            "          [-0.5686, -0.5686, -0.5686,  ..., -0.5843, -0.5843, -0.5843],\n",
            "          [-0.5686, -0.5686, -0.5686,  ..., -0.5843, -0.5843, -0.5843],\n",
            "          [-0.5686, -0.5686, -0.5686,  ..., -0.5843, -0.5843, -0.5843]],\n",
            "\n",
            "         [[-0.3098, -0.3098, -0.3098,  ..., -0.3289, -0.3669, -0.3731],\n",
            "          [-0.3046, -0.3046, -0.3046,  ..., -0.3216, -0.3538, -0.3590],\n",
            "          [-0.2932, -0.2932, -0.2932,  ..., -0.3059, -0.3255, -0.3285],\n",
            "          ...,\n",
            "          [-0.2627, -0.2627, -0.2627,  ..., -0.2941, -0.2941, -0.2941],\n",
            "          [-0.2627, -0.2627, -0.2627,  ..., -0.2941, -0.2941, -0.2941],\n",
            "          [-0.2627, -0.2627, -0.2627,  ..., -0.2941, -0.2941, -0.2941]],\n",
            "\n",
            "         [[-0.4196, -0.4196, -0.4196,  ..., -0.3877, -0.3754, -0.3658],\n",
            "          [-0.4144, -0.4144, -0.4144,  ..., -0.3908, -0.3800, -0.3677],\n",
            "          [-0.4030, -0.4030, -0.4030,  ..., -0.3976, -0.3900, -0.3718],\n",
            "          ...,\n",
            "          [-0.4275, -0.4275, -0.4275,  ..., -0.4275, -0.4275, -0.4275],\n",
            "          [-0.4275, -0.4275, -0.4275,  ..., -0.4275, -0.4275, -0.4275],\n",
            "          [-0.4275, -0.4275, -0.4275,  ..., -0.4275, -0.4275, -0.4275]]]])\n",
            "tensor([0, 3, 1, 0, 0, 2, 0, 0, 0, 0, 3, 0, 1, 1, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZwQCe2Sckz-"
      },
      "source": [
        "def train_val(model, params):\n",
        "  # wandb.init(project='test', entity='jeeeeeeeee') # 추가된 코드 1\n",
        "  # wandb.watch(ResNet, log='all', log_freq=10) # 추가된 코드 2\n",
        "\n",
        "\n",
        "  num_epochs = params['num_epochs']\n",
        "  loss_func = params['loss_func']\n",
        "  opt = params['optimizer']\n",
        "  train_dl = params['train_dl']\n",
        "  val_dl = params['val_dl']\n",
        "  sanity_check = params['sanity_check']\n",
        "  lr_scheduler = params['lr_scheduler']\n",
        "  path2weights = params['path2weights']\n",
        "\n",
        "  loss_history = { 'train': [], 'val': [] }\n",
        "  metric_history = { 'train': [], 'val': [] } \n",
        "\n",
        "\n",
        "  # GPU out of memory error\n",
        "  # best_model_wts = oopy.deepoopy(model.state_dict())\n",
        "\n",
        "  # best_loss = float('inf')\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(num_epochs): # epoch 안에 \n",
        "    current_lr = get_lr(opt)\n",
        "    print('Epoch {}/{}, current lr = {}'.format(epoch, num_epochs-1, current_lr))\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    print('model training finished') ############################################ print\n",
        "    \n",
        "    # loss_arr += [loss.item()]\n",
        "\n",
        "    # print(\"Batch Loss : %.4f\" % np.mean(loss_arr))\n",
        "\n",
        "    # wandb.log({'Epoch': epoch, 'loss': np.mean(loss_arr)}) # 추가된 코드 3\n",
        "\n",
        "    # train_loss, train_metric = loss_per_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
        "    # train_loss = loss_per_batch(loss_func, train_dl, opt)\n",
        "    # print('loss_per_epoch finished') ########################################## print\n",
        "    # loss_history['train'].append(train_loss)\n",
        "    # metric_history['train'].append(train_metric)\n",
        "\n",
        "    # model.eval()\n",
        "    # print('model.eval() finished') ############################################### print\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #   val_loss, val_metric = loss_per_epoch(model, loss_func, val_dl, sanity_check)\n",
        "    # loss_history['val'].append(val_loss)\n",
        "    # metric_history['val'].append(val_metric)\n",
        "\n",
        "    # if val_loss < best_loss:\n",
        "    #   best_loss = val_loss\n",
        "    #   # best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    #   # torch.save(model.state_dict(), path2weights)\n",
        "    #   # print('Copied best model weights!')\n",
        "    #   print('Got best val_loss')\n",
        "\n",
        "    # lr_scheduler.step(val_loss)\n",
        "    # # wandb.log({'Epoch': epoch, 'loss': np.mean(loss_arr)}) # 추가된 코드 3\n",
        "    # print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
        "    # print('-'*10)\n",
        "\n",
        "  # return model, loss_history, metric_history\n",
        "    return model, loss_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "UZ1xUU1kuIiO",
        "outputId": "2f19e681-29ee-459c-f412-d74209f73123"
      },
      "source": [
        "model, loss_hist, metric_hist = train_val(model, params_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/1, current lr = 0.001\n",
            "model training finished\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-40a68c4f9fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-41af6d50012d>\u001b[0m in \u001b[0;36mtrain_val\u001b[0;34m(model, params)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mloss_arr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch Loss : %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5hBNS4_dd0x"
      },
      "source": [
        "# 하이퍼 파라미터 정의\n",
        "\n",
        "params_train = {\n",
        "    'num_epochs': 2,\n",
        "    'optimizer' : opt,\n",
        "    'loss_func' : loss_func,\n",
        "    'train_dl' : train_dl,\n",
        "    'val_dl' : val_dl,\n",
        "    'sanity_check' : False,\n",
        "    'lr_scheduler' : lr_scheduler,\n",
        "    'path2weights' : './models/weights.pt',\n",
        "}\n",
        "\n",
        "def createFolder(directory):\n",
        "  try:\n",
        "    if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "  except OSerror:\n",
        "    print('Error')\n",
        "\n",
        "createFolder('./models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "sF12rQJ_dgd1",
        "outputId": "1fa94738-6d49-4af0-f179-a4d0777fd517"
      },
      "source": [
        "model, loss_hist, metric_hist = train_val(model, params_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/1, current lr = 0.001\n",
            "model training finished\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-40a68c4f9fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-a4ab4be3d594>\u001b[0m in \u001b[0;36mtrain_val\u001b[0;34m(model, params)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# wandb.log({'Epoch': epoch, 'loss': np.mean(loss_arr)}) # 추가된 코드 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_per_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanity_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss_per_epoch finished'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m########################################## print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ce65f08b6c58>\u001b[0m in \u001b[0;36mloss_per_epoch\u001b[0;34m(model, loss_func, dataset_dl, sanity_check, opt)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mloss_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_per_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ce65f08b6c58>\u001b[0m in \u001b[0;36mloss_per_batch\u001b[0;34m(loss_func, output, target, opt)\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1U9fG4AuATI"
      },
      "source": [
        "# 데이터 넣으면 왜 안될까\n",
        "\n",
        "1. dataset 불러오고 이거 training 시키는 건 ㄹㅇ 10초..\n",
        "2. dataset 불러오고 세가지 정도의 방법으로 train test split 해봤는데 자꾸 에러뜸 -> 해결!\n",
        "- 자꾸 사이즈가 안맞대요!! Resize가 일을 안하는 것 같은데 왜 안하는지 모르겠어요 ㅠㅠㅠ\n",
        "3. 그냥 loss만 찍으면 무한루프 돌기 시작함\n",
        "4. STL 은 도대체 왜 되는 거임?ㅎ"
      ]
    }
  ]
}